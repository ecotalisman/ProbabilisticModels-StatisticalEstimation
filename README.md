# Probabilistic models and statistical estimation in information systems

Work performed using the Python programming language and necessary libraries in the Jupyter Notebook (IDE) environment.
Data for analysis was taken from the following link:
https://data.worldbank.org/country/UA?locale=uk

To form the data, a data filter was applied according to the criteria above and a necessary file in CSV format was created. Countries for which it was not possible to obtain the necessary data for the selected period were removed from the sample at the stage of data formation for analysis.

The aim of this analysis is:

1) For `labs_M_military`, to perform a comparison of military expenditures based on main economic indicators to get a general picture regarding the efficiency of most countries' peacekeeping policy. It is planned to highlight certain clusters of countries that spend significant funds on defense depending on GDP.

2) For `labs_R_birthRate`, to obtain results regarding the verification of the hypothesis that a high degree of economic development does not promote an increase in birth rate.

The k-means MacQueen method was used for the analysis.

The problem of partitioning n objects into k (k < n) homogeneous clusters in a certain sense is solved. At the initial stage of its implementation, the initial points are arranged (possibly randomly) and the first k points are subsequently considered as separate clusters, which are given unit weight coefficients. Then, point Xk+1 is taken and it is determined which of the existing clusters it is closest to. This cluster is replaced with a new one, located at the center of gravity of the initial cluster and point Xk+1. At the same time, the weight coefficient of the resulting cluster is increased by one unit compared to the weight coefficient of the initial one. If point Xk+1 is equidistant from several clusters, it is included in the cluster with the smallest number or with the largest weight coefficient. Then, the remaining points are sequentially joined to the existing clusters. With sufficiently large volumes of the studied selections, the centers of gravity of the obtained clusters eventually stop changing, i.e., the iterative procedure converges to a certain limit. If it does not converge within a given number of steps, one of the following methods is used. The first implies that after considering the last point Xn, they return to points X1, X2, etc. The second approach involves repeatedly selecting initial clusters. At each stage, points that are closest to the final clusters most often obtained in previous stages are chosen as the initial ones.

The method's peculiarity is the algorithmic guarantee that each of the classified objects will be counted only into one of the clusters. When applying this method, there is no special need for visualization of results. However, for clarity, it can be carried out by displaying spatial ellipsoids containing classified objects (if the dimension does not exceed three), or two-dimensional slices of space. In many cases, the k-means method makes it possible to obtain a partition close to the best in terms of the quality functional.
If the number of classes is unknown, two constants need to be set: the coarseness measure φ and the accuracy measure ψ. At the zeroth step, an arbitrary value of the number of classes `k0` is taken. The initial points are arranged in order and the first `k0` points are considered as the centers of clusters, to which unit weight coefficients are assigned. Then, the initial clusters are coarsened. For this, pairwise distances between them are sequentially calculated and, if the distance between two clusters does not exceed φ, they are united into one. This new cluster is their weighted average and has a weight coefficient equal to the sum of the weight coefficients of the initial clusters. After completing this procedure, we get `k0*` ≤ `k0` clusters.

Further, the remaining points are sequentially distributed over the clusters. For each point, the cluster closest to it is determined. If the distance between them does not exceed ψ, then the corresponding point is attached to this cluster according to the procedure mentioned above. Otherwise, it is considered the center of a new cluster, which is given a unit weight coefficient. After distributing all the points over the clusters, the coarsening procedure is repeated and we move on to the next iteration step.

By choosing different values of the constants φ and ψ, one can obtain different partitions of the original set. The choice is considered satisfactory if the result of classification is close to optimal according to expert estimates or in terms of the quality functional. It can be proven that the algorithm of the MacQueen method converges to the local minimum of the sum of intra-class variances. The global minimum of this functional can be achieved using R. Jensen's algorithm, which is based on the application of dynamic programming.
